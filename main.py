import streamlit as st
from rag_bk.bk_logging import langsmith
from dotenv import load_dotenv
from rag_bk.modules.handler import stream_handler
# from sidebar import show_sidebar
from st_function import print_messages, add_message
from rag_bk.bk_messages import random_uuid


# API KEY ì •ë³´ë¡œë“œ
api_key = st.secrets["OPENAI_API_KEY"]

# í”„ë¡œì íŠ¸ ì´ë¦„
langsmith("ì±—ë´‡ìƒë‹´")

st.title("The Health Curator ğŸ’¬11")
 
# ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
if st.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”"):
    st.session_state["messages"] = []  # ëŒ€í™” ì •ë³´ ì§€ìš°ê¸°
    st.session_state["thread_id"] = random_uuid()  # ì‚¬ìš©ìì •ë³´ ê¸°ì–µ ì§€ìš°ê¸°

# ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ReAct Agent ì´ˆê¸°í™”
if "react_agent" not in st.session_state:
    st.session_state["react_agent"] = None

# ì‚¬ì´ë“œë°” ìƒì„±(sidebar.pyë¡œ ìƒì„±)
# show_sidebar()

import streamlit as st
from langchain_core.prompts import load_prompt, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from rag_bk.modules.tools import WebSearchTool, retriever_tool
from rag_bk.modules.agent import create_agent_executor
from rag_bk.bk_messages import random_uuid
# step 1 LLMì„ í†µí•œ í˜ë¥´ì†Œë‚˜ë¥¼ ë¶€ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±í•˜ê¸°
# gen_prom.yaml ë¡œë“œ(ê°œì¸ì •ë³´ì— ë§ëŠ” í˜ë¥´ì†Œë‚˜ë¥¼ ë¶€ì—¬í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸)
loaded_prompt = load_prompt("prompts/gen_prom.yaml", encoding="utf-8")

# ìµœì¢… í˜ë¥´ì†Œë‚˜ ë¶€ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
final_template = (
    f"{loaded_prompt.template}, "  # ì‚¬ìš©ì ìƒë‹´ë‚´ìš© + ê°œì¸ì •ë³´ì— ë§ëŠ” í˜ë¥´ì†Œë‚˜ë¥¼ ë¶€ì—¬í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
)

# í˜ë¥´ì†Œë‚˜ ìƒì„± LLM í˜¸ì¶œ (ì²« ë²ˆì§¸ ì²´ì¸)
# prompt1 = PromptTemplate.from_template(template=final_template)
# llm = ChatOpenAI(api_key=api_key, model_name="gpt-4o", temperature=0.5)
# chain1 = prompt1 | llm | StrOutputParser()
# st.session_state["new_prompt"] = chain1.invoke(
#     ""
# )

st.session_state["new_prompt"] = final_template

# step 2 ìƒì„±ëœ í˜ë¥´ì†Œë‚˜ë¥¼ ì´ìš©í•œ ìµœì¢… ë‹µë³€ LLM ìƒì„±
tool1 = retriever_tool()  # pdf_search

# WebSearchTool ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
tool2 = WebSearchTool().create()  # web_search

# ë¦¬ì•¡íŠ¸í˜• ì—ì´ì „íŠ¸ ìƒì„±
st.session_state["react_agent"] = create_agent_executor(
    model_name="gpt-4o-mini",
    tools=[tool1, tool2],
)
# ê³ ìœ  ìŠ¤ë ˆë“œ ID(ëœë¤ìœ¼ë¡œ ì§€ì–´ì£¼ê¸° -> ëŒ€í™” ê¸°ì–µìš©ë„ -> ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”í•˜ë©´ ì´ê²ƒë„ ì´ˆê¸°í™”)
st.session_state["thread_id"] = random_uuid()

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥(st_function.pyë¡œ ìƒì„±)
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    agent = st.session_state["react_agent"]
    # Config ì„¤ì •

    if agent is not None:
        config = {"configurable": {"thread_id": st.session_state["thread_id"]}}
        # ì‚¬ìš©ìì˜ ì…ë ¥
        st.chat_message("user").write(user_input)

        with st.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()

            ai_answer = ""
            container_messages, tool_args, agent_answer = stream_handler(
                container,
                agent,
                {
                    "messages": [
                        ("human", user_input),
                    ]
                },
                config,
            )

            # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
            add_message("user", user_input)
            for tool_arg in tool_args:
                add_message(
                    "assistant",
                    tool_arg["tool_result"],
                    "tool_result",
                    tool_arg["tool_name"],
                )
            add_message("assistant", agent_answer)
    else:
        warning_msg.warning("ì‚¬ì´ë“œë°”ì—ì„œ ê°œì¸ì •ë³´ ì…ë ¥ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
